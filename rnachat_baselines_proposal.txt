# RNAChat Baseline Models - Comprehensive Proposal

## Executive Summary

This proposal outlines 15 baseline models across 6 categories to provide robust comparisons for RNAChat. These baselines will demonstrate the superiority of your multi-modal architecture across different methodological paradigms.

---

## 1. Traditional Machine Learning Baselines

### 1.1 TF-IDF + Random Forest (TF-IDF-RF)
**Methodology:**
- Extract k-mer features (k=3,4,5) from RNA sequences
- Apply TF-IDF vectorization
- Train Random Forest classifier for function categories
- Generate text from predicted categories using templates

**Expected Performance:** Low (BLEU: 0.10-0.15)
**Purpose:** Demonstrates need for deep learning

### 1.2 TF-IDF + SVM (TF-IDF-SVM)
**Methodology:**
- Similar to RF but uses SVM with RBF kernel
- Multi-class classification with one-vs-rest strategy

**Expected Performance:** Low (BLEU: 0.12-0.16)
**Purpose:** Traditional ML upper bound

---

## 2. Sequence-Only Deep Learning Baselines

### 2.1 LSTM Encoder-Decoder (LSTM-ED)
**Architecture:**
- Encoder: 2-layer bidirectional LSTM (hidden=512)
- Decoder: 2-layer LSTM with attention mechanism
- Token embedding: 256-dim
- Direct sequence-to-text generation

**Expected Performance:** Medium (BLEU: 0.15-0.20)
**Purpose:** Shows value of pre-trained RNA encoders

### 2.2 GRU Encoder-Decoder (GRU-ED)
**Architecture:**
- Similar to LSTM but using GRU cells
- Faster training, comparable performance

**Expected Performance:** Medium (BLEU: 0.14-0.19)
**Purpose:** Alternative RNN architecture

### 2.3 Transformer Encoder-Decoder (Trans-ED)
**Architecture:**
- 6-layer encoder, 6-layer decoder
- 8 attention heads, d_model=512
- Custom RNA tokenization
- Trained from scratch on your dataset

**Expected Performance:** Medium-High (BLEU: 0.18-0.22)
**Purpose:** Modern architecture without pre-training

### 2.4 CNN-LSTM Hybrid (CNN-LSTM)
**Architecture:**
- CNN feature extractor (3 conv layers: 256, 512, 512)
- LSTM decoder with attention
- Captures local motifs + sequential dependencies

**Expected Performance:** Medium (BLEU: 0.16-0.20)
**Purpose:** Alternative feature extraction approach

---

## 3. Pre-trained Language Model Baselines

### 3.1 Fine-tuned T5-Base (FT-T5-Base)
**Methodology:**
- T5-base (220M parameters)
- Input: "describe RNA function: [sequence]"
- Fine-tune on your dataset (encoder + decoder)

**Expected Performance:** Medium-High (BLEU: 0.19-0.23)
**Purpose:** Comparable-size pre-trained baseline

### 3.2 Fine-tuned T5-Large (FT-T5-Large)
**Methodology:**
- T5-large (770M parameters)
- Same setup as T5-base

**Expected Performance:** High (BLEU: 0.21-0.25)
**Purpose:** Larger pre-trained model comparison

### 3.3 Fine-tuned FLAN-T5-Base (FT-FLAN-T5-Base)
**Methodology:**
- FLAN-T5-base (instruction-tuned T5)
- Better instruction following capabilities

**Expected Performance:** Medium-High (BLEU: 0.20-0.24)
**Purpose:** Instruction-tuned variant

### 3.4 Fine-tuned BART-Base (FT-BART-Base)
**Methodology:**
- BART-base denoising autoencoder
- Fine-tune for RNA function generation

**Expected Performance:** Medium-High (BLEU: 0.18-0.22)
**Purpose:** Alternative pre-training objective

---

## 4. Alternative RNA Encoder Baselines

### 4.1 RNA-FM + Adaptor + Vicuna (RNA-FM-Chat)
**Architecture:**
- Replace RiNALMo with RNA-FM encoder
- Keep same adaptor and Vicuna setup
- Same 2-stage training

**Expected Performance:** High (BLEU: 0.20-0.24)
**Purpose:** Direct encoder comparison (you have AIDO.RNA already)

### 4.2 One-Hot + MLP + Vicuna (OneHot-Chat)
**Architecture:**
- One-hot encoding of RNA sequences
- 3-layer MLP (1024 → 2048 → 5120)
- Connect to Vicuna via adaptor

**Expected Performance:** Low-Medium (BLEU: 0.12-0.17)
**Purpose:** Shows value of sophisticated RNA encoders

---

## 5. Retrieval-Based Baselines

### 5.1 k-NN Retrieval (kNN-Retrieval)
**Methodology:**
- Compute sequence similarity (edit distance / embedding cosine)
- Retrieve k=5 most similar training examples
- Return concatenated/averaged function descriptions

**Expected Performance:** Low-Medium (BLEU: 0.14-0.18, High SimCSE: 0.75-0.80)
**Purpose:** Simple but strong baseline

### 5.2 RAG with GPT-4o (RAG-GPT4o)
**Methodology:**
- Retrieve k=3 similar RNAs with functions
- Prompt: "Given these similar RNAs: [examples], describe function of: [query]"
- Use GPT-4o for generation

**Expected Performance:** Medium-High (BLEU: 0.18-0.22)
**Purpose:** Shows value of end-to-end learning

### 5.3 RAG with LLaMA-2 (RAG-LLaMA2)
**Methodology:**
- Same as RAG-GPT4o but with LLaMA-2-13B
- More controlled comparison

**Expected Performance:** Medium (BLEU: 0.17-0.21)
**Purpose:** Open-source RAG baseline

---

## 6. Hybrid and Ablation Baselines

### 6.1 RiNALMo + Template Generation (RiNALMo-Template)
**Methodology:**
- RiNALMo embeddings → MLP classifier
- Predict function categories
- Use rule-based templates to generate text

**Expected Performance:** Medium (BLEU: 0.16-0.20)
**Purpose:** Shows value of free-form generation

---

## Implementation Timeline

| Week | Task |
|------|------|
| 1 | Traditional ML baselines (1.1-1.2) + k-NN (5.1) |
| 2 | Seq2seq models (2.1-2.4) |
| 3 | T5/FLAN-T5 variants (3.1-3.3) |
| 4 | Alternative encoders (4.1-4.2) + RAG (5.2-5.3) |
| 5 | Evaluation, analysis, table/figure generation |

---

## Expected Results Table Structure

| Model | Type | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 | SimCSE | Params |
|-------|------|--------|--------|--------|--------|--------|--------|
| TF-IDF-RF | Traditional ML | ~0.12 | ~0.06 | ~0.03 | ~0.01 | ~0.65 | <1M |
| LSTM-ED | Seq2Seq | ~0.18 | ~0.10 | ~0.06 | ~0.03 | ~0.72 | 50M |
| Trans-ED | Transformer | ~0.20 | ~0.11 | ~0.07 | ~0.04 | ~0.75 | 80M |
| FT-T5-Base | Pre-trained LM | ~0.21 | ~0.12 | ~0.07 | ~0.04 | ~0.77 | 220M |
| kNN-Retrieval | Retrieval | ~0.16 | ~0.09 | ~0.05 | ~0.03 | ~0.78 | 0 |
| RAG-GPT4o | Retrieval+LLM | ~0.20 | ~0.11 | ~0.07 | ~0.04 | ~0.76 | - |
| **RNAChat** | **Multi-modal** | **0.252** | **0.143** | **0.091** | **0.056** | **0.796** | **650M+13B** |

---

## Key Advantages to Highlight

1. **vs Traditional ML**: RNAChat achieves 2x BLEU scores, showing deep learning necessity
2. **vs Seq2Seq**: Pre-trained encoders crucial (+30% performance)
3. **vs T5 models**: Domain-specific RNA encoders outperform general-purpose pre-training
4. **vs Retrieval**: End-to-end learning better than nearest neighbor matching
5. **vs RAG**: Integrated architecture superior to retrieval augmentation

---

## Computational Requirements

- **Total GPU hours**: ~500-800 hours (A100)
- **Storage**: ~100GB for model checkpoints
- **Training data**: Your existing 4,605 RNA dataset
- **Inference time**: Vary from <1ms (kNN) to 2s (RNAChat)

---

## Statistical Significance Testing

For each baseline:
1. Run 3 random seeds
2. Report mean ± std
3. Perform paired t-tests vs RNAChat
4. Report p-values with Bonferroni correction

---

## Manuscript Updates

### New Sections to Add:

1. **Extended Related Work**: Categorize prior work by approach type
2. **Baseline Methods**: Describe each category briefly
3. **Extended Results**: Multi-panel figure comparing all baselines
4. **Analysis**: Why does RNAChat outperform each category?

### New Figures:

1. **Figure X**: Comprehensive bar chart (all models, all metrics)
2. **Figure Y**: Performance vs model size scatter plot
3. **Figure Z**: Qualitative comparison (same RNA, different models)

---

## Code Organization

```
rnachat_baselines/
├── data/
│   └── data_loader.py          # Unified data loading
├── models/
│   ├── traditional_ml/
│   │   ├── tfidf_rf.py
│   │   └── tfidf_svm.py
│   ├── seq2seq/
│   │   ├── lstm_ed.py
│   │   ├── gru_ed.py
│   │   ├── transformer_ed.py
│   │   └── cnn_lstm.py
│   ├── pretrained_lm/
│   │   ├── finetune_t5.py
│   │   ├── finetune_flant5.py
│   │   └── finetune_bart.py
│   ├── alternative_encoders/
│   │   ├── rnafm_chat.py
│   │   └── onehot_chat.py
│   ├── retrieval/
│   │   ├── knn_retrieval.py
│   │   ├── rag_gpt4o.py
│   │   └── rag_llama2.py
│   └── hybrid/
│       └── rinalmo_template.py
├── training/
│   ├── train_seq2seq.py
│   ├── train_lm.py
│   └── train_multimodal.py
├── evaluation/
│   ├── evaluate.py
│   ├── metrics.py
│   └── statistical_tests.py
├── utils/
│   ├── tokenization.py
│   ├── sequence_utils.py
│   └── logging.py
└── configs/
    └── [model_configs].yaml
```
